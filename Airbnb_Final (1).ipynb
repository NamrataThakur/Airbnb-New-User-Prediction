{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Notebook for Prediction :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from datetime import datetime, date\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "#from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelBinarizer,LabelEncoder\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import joblib\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the vectorizers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_gender = pickle.load(open(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\final_dataset\\vect_gender.pickle', 'rb'))\n",
    "vect_signup_method = pickle.load(open(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\final_dataset\\vect_signup_method.pickle', 'rb'))\n",
    "vect_language = pickle.load(open(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\final_dataset\\vect_language.pickle', 'rb'))\n",
    "\n",
    "vect_affiliate_channel = pickle.load(open(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\final_dataset\\vect_affiliate_channel.pickle', 'rb'))\n",
    "vect_affiliate_provider = pickle.load(open(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\final_dataset\\vect_affiliate_provider.pickle', 'rb'))\n",
    "vect_first_affiliate_tracked = pickle.load(open(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\final_dataset\\vect_first_affiliate_tracked.pickle', 'rb'))\n",
    "\n",
    "vect_signup_app = pickle.load(open(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\final_dataset\\vect_signup_app.pickle', 'rb'))\n",
    "vect_first_device_type = pickle.load(open(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\final_dataset\\vect_first_device_type.pickle', 'rb'))\n",
    "vect_first_browser = pickle.load(open(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\final_dataset\\vect_first_browser.pickle', 'rb'))\n",
    "\n",
    "vect_unique_device_type = pickle.load(open(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\final_dataset\\vect_unique_device_type.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_action = pickle.load(open(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\final_dataset\\tfidf_vect_action.pickle', 'rb'))\n",
    "tfidf_vect_action_type = pickle.load(open(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\final_dataset\\tfidf_vect_action_type.pickle', 'rb'))\n",
    "tfidf_vect_action_detail = pickle.load(open(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\final_dataset\\tfidf_vect_action_detail.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\final_dataset\\final_column_list.txt', 'rb') as fp:   # Unpickling\n",
    "    final_column_list = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "Y = pd.read_csv(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\test_users.csv\\raw_target.csv')\n",
    "train_session_raw_df = pd.read_csv(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\test_users.csv\\train_session_raw_df.csv')\n",
    "\n",
    "#Loading the RF Model :\n",
    "clf_rf = joblib.load(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\models\\clf_rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\train_users_2.csv\\train_users_2.csv')\n",
    "session_df = pd.read_csv(r'C:\\Users\\NamrataT\\Desktop\\CS_1\\Dataset\\airbnb-recruiting-new-user-bookings\\sessions.csv\\sessions.csv')\n",
    "session_df_unq_rec1 = session_df.groupby('user_id', as_index=False).agg(lambda x: x.tolist())\n",
    "train_session_df = train_df.merge(session_df_unq_rec1, left_on = 'id', right_on = 'user_id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_session_df = train_df.merge(session_df_unq_rec1, left_on = 'id', right_on = 'user_id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'date_account_created', 'timestamp_first_active',\n",
       "       'date_first_booking', 'gender', 'age', 'signup_method', 'signup_flow',\n",
       "       'language', 'affiliate_channel', 'affiliate_provider',\n",
       "       'first_affiliate_tracked', 'signup_app', 'first_device_type',\n",
       "       'first_browser', 'country_destination', 'user_id', 'action',\n",
       "       'action_type', 'action_detail', 'device_type', 'secs_elapsed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_session_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_session_df_backup = train_session_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creation of Raw data for the Final Method :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = train_session_df[:5]\n",
    "Y_raw = Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 7, 7, ..., 7, 7, 7])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining the functions used :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Scoring function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/davidgasquez/ndcg-scorer\n",
    "\n",
    "def dcg_score(y_true, y_score, k=5):\n",
    "    \n",
    "    \"\"\"Discounted cumulative gain (DCG) at rank K.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array, shape = [n_samples]\n",
    "        Ground truth (true relevance labels).\n",
    "    y_score : array, shape = [n_samples, n_classes]\n",
    "        Predicted scores.\n",
    "    k : int\n",
    "        Rank.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "    \"\"\"\n",
    "    \n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    gain = 2 ** y_true - 1\n",
    "\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(ground_truth, predictions, k=5):\n",
    "    \n",
    "    \"\"\"Normalized discounted cumulative gain (NDCG) at rank K.\n",
    "\n",
    "    Normalized Discounted Cumulative Gain (NDCG) measures the performance of a\n",
    "    recommendation system based on the graded relevance of the recommended\n",
    "    entities. It varies from 0.0 to 1.0, with 1.0 representing the ideal\n",
    "    ranking of the entities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ground_truth : array, shape = [n_samples]\n",
    "        Ground truth (true labels represended as integers).\n",
    "    predictions : array, shape = [n_samples, n_classes]\n",
    "        Predicted probabilities.\n",
    "    k : int\n",
    "        Rank.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> ground_truth = [1, 0, 2]\n",
    "    >>> predictions = [[0.15, 0.55, 0.2], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]\n",
    "    >>> score = ndcg_score(ground_truth, predictions, k=2)\n",
    "    1.0\n",
    "    >>> predictions = [[0.9, 0.5, 0.8], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]\n",
    "    >>> score = ndcg_score(ground_truth, predictions, k=2)\n",
    "    0.6666666666\n",
    "    \"\"\"\n",
    "    \n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(range(predictions.shape[1] + 1))\n",
    "    T = lb.transform(ground_truth)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    # Iterate over each y_true and compute the DCG score\n",
    "    for y_true, y_score in zip(T, predictions):\n",
    "        actual = dcg_score(y_true, y_score, k)\n",
    "        best = dcg_score(y_true, y_true, k)\n",
    "        score = float(actual) / float(best)\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# NDCG Scorer function\n",
    "ndcg_scorer = make_scorer(ndcg_score, needs_proba=True, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Age processing function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to replace any value of age less than 15 or greater than 2007 with the median age\n",
    "\n",
    "def func_age_imput_median(age):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to replace any value of age less than 15 or greater than 2007 with the median age\n",
    "    \n",
    "    parameters: age \n",
    "    \n",
    "    returns : age  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if age < 15.0 or age > 2007.0:\n",
    "        return 34.0\n",
    "    else:\n",
    "        return age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_age_imput_year(age,year):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to check if age is between 117 and 2007. If true, it will substract that age value from the year of account \n",
    "    creation to get the exact age of the user on the year he created the account.\n",
    "        \n",
    "    parameters: age,account_created_year \n",
    "    \n",
    "    returns : age  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    res = 0\n",
    "    if age > 117.0 and age <=2007.0:\n",
    "        res = year-age\n",
    "        return res \n",
    "    else:\n",
    "        return age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Session data processing functions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_sec(sec_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate total time a particular user has spent in accessing the application.\n",
    "        \n",
    "    parameters: secs_elapsed \n",
    "    \n",
    "    returns : total_secs_elapsed  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    #print(sec_list)\n",
    "    \n",
    "    \n",
    "    #need to convert each element of the list to string so as to replace the 'nan' with '0' so that we dont get ValueError later.\n",
    "    sec_list = [ str(i) for i in sec_list ] \n",
    "    sec_list = [ re.sub('nan','0',i) for i in sec_list] \n",
    "    \n",
    "    #sec_list is a list of strings now. Iterating over the elements of the list.\n",
    "    for i in sec_list:\n",
    "        \n",
    "        #print(i)\n",
    "        #Converting the string to float to take the sum later\n",
    "        res.append(float(i)) \n",
    "    res = sum(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def average_sec(sec_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate average time a particular user has spent in accessing the application.\n",
    "        \n",
    "    parameters: secs_elapsed \n",
    "    \n",
    "    returns : mean_secs_elapsed  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    #need to convert each element of the list to string so as to replace the 'nan' with '0' so that we dont get ValueError later.\n",
    "    sec_list = [ str(i) for i in sec_list ] \n",
    "    sec_list = [ re.sub('nan','0',i) for i in sec_list] \n",
    "    \n",
    "    #session_df is a list of strings now. Iterating over the elements of the list.\n",
    "    for i in sec_list:\n",
    "        \n",
    "        #Converting the string to float to take the mean later\n",
    "        res.append(float(i)) \n",
    "    res = mean(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_count(sec_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate total sessions a particular user has used in accessing the application.\n",
    "        \n",
    "    parameters: secs_elapsed \n",
    "    \n",
    "    returns : session_count  \n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    return len(sec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_action(action):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate unique actions per user.\n",
    "        \n",
    "    parameters: action/device_* \n",
    "    \n",
    "    returns : unique_action/device_*  \n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "    action = [str(i) for i in action]\n",
    "    \n",
    "    action = [re.sub('nan','na',i) for i in action]\n",
    "    \n",
    "    action = ','.join(set(action))\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING THE ENTIRE RAW DATA :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to preprocess the raw data.\n",
    "    \n",
    "    It performs the following tasks :\n",
    "    \n",
    "    a) Removing outliers\n",
    "    b) Filling Missing Values\n",
    "    c) Finding unique values in the sessions columns\n",
    "    d) Computing sum/mean for the secs_elapsed column\n",
    "    e) Dropping the columns not needed\n",
    "    \n",
    "    parameters: raw data\n",
    "    \n",
    "    returns : processed data  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Processing 'date_account_created' :\n",
    "    data['date_account_created'] = pd.to_datetime(data['date_account_created'])\n",
    "    data['account_created_day'] = data['date_account_created'].dt.weekday\n",
    "    data['account_created_month'] = data['date_account_created'].dt.month\n",
    "    data['account_created_year'] = data['date_account_created'].dt.year\n",
    "    \n",
    "    print('\"date_account_created\" field is finished processing...')\n",
    "    \n",
    "    #Processing 'age' :\n",
    "    data['age'] = data['age'].fillna(34.0)\n",
    "    data['age'] = data['age'].apply(func_age_imput_median)\n",
    "    data['age'] = data.apply(lambda x: func_age_imput_year(x['age'], x['account_created_year']), axis=1)\n",
    "    \n",
    "    print('\"age\" field is finished processing...')\n",
    "    \n",
    "    data['first_affiliate_tracked'] = data['first_affiliate_tracked'].fillna('untracked')\n",
    "    \n",
    "    print('\"first_affiliate_tracked\" field is finished processing...')\n",
    "    \n",
    "    data['Total_secs_elapsed'] = data['secs_elapsed'].apply(total_sec)\n",
    "    data['Mean_secs_elapsed'] = data['secs_elapsed'].apply(average_sec)\n",
    "    data['session_count'] = data['secs_elapsed'].apply(session_count)\n",
    "    data['unique_action'] = data['action'].apply(unique_action)\n",
    "    data['unique_action_type'] = data['action_type'].apply(unique_action)\n",
    "    data['unique_action_detail'] = data['action_detail'].apply(unique_action)\n",
    "    data['unique_device_type'] = data['device_type'].apply(unique_action)\n",
    "    \n",
    "    print('session fields are finished processing...')\n",
    "    \n",
    "    data = data.drop(['date_first_booking','timestamp_first_active','id','user_id','action','action_type',\n",
    "                      'action_detail','device_type','secs_elapsed','date_account_created'],axis = 1, inplace = True)\n",
    "    \n",
    "    print('Fields already processed are dropped...')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VECTORISATION OF THE ENTIRE RAW DATA :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise_data(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to vectorise the processed data and \n",
    "    stacking the sparse columns together to create the final data matrix.\n",
    "    \n",
    "    parameters: processed data\n",
    "    \n",
    "    returns :  vectorised data  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    test_session_gender_bow = vect_gender.transform(data['gender'].values)\n",
    "    print('Gender column vectorisation finished...')\n",
    "    test_session_signup_method_bow = vect_signup_method.transform(data['signup_method'].values)\n",
    "    print('signup_method column vectorisation finished...')\n",
    "    test_session_language_bow = vect_language.transform(data['language'].values)\n",
    "    print('language column vectorisation finished...')\n",
    "    test_session_affiliate_channel_bow = vect_affiliate_channel.transform(data['affiliate_channel'].values)\n",
    "    print('affiliate_channel column vectorisation finished...')\n",
    "    test_session_affiliate_provider_bow = vect_affiliate_provider.transform(data['affiliate_provider'].values)\n",
    "    print('affiliate_provider column vectorisation finished...')\n",
    "    test_session_first_affiliate_tracked_bow = vect_first_affiliate_tracked.transform(data['first_affiliate_tracked'].values)\n",
    "    print('first_affiliate_tracked column vectorisation finished...')\n",
    "    test_session_signup_app_bow = vect_signup_app.transform(data['signup_app'].values)\n",
    "    print('signup_app column vectorisation finished...')\n",
    "    test_session_first_device_type_bow = vect_first_device_type.transform(data['signup_app'].values)\n",
    "    print('signup_app column vectorisation finished...')\n",
    "    test_session_first_browser_bow = vect_first_browser.transform(data['first_browser'].values)\n",
    "    print('first_browser column vectorisation finished...')\n",
    "    test_session_unique_device_type_bow = vect_unique_device_type.transform(data['unique_device_type'].values)\n",
    "    print('unique_device_type column vectorisation finished...')\n",
    "    test_session_action_tfidf = tfidf_vect_action.transform(data['unique_action'].values)\n",
    "    print('unique_action column vectorisation finished...')\n",
    "    test_session_action_type_tfidf = tfidf_vect_action_type.transform(data['unique_action_type'].values)\n",
    "    print('unique_action_type column vectorisation finished...')\n",
    "    test_session_action_detail_tfidf = tfidf_vect_action_detail.transform(data['unique_action_detail'].values)\n",
    "    print('unique_action_detail column vectorisation finished...')\n",
    "    \n",
    "    print('Current Columns : ')\n",
    "    print(data.columns)\n",
    "    data = data.drop(['gender', 'country_destination','signup_method', 'language', 'affiliate_channel','affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser','unique_action','unique_action_type','unique_action_detail','unique_device_type'],axis=1)\n",
    "    print('Dropping columns that are vectorised : ')\n",
    "    print(data.columns)\n",
    "    print(data.shape)\n",
    "    \n",
    "    # Stacking the numerical columns with the vectorised columns to create the final data matrix :\n",
    "    data = sparse.hstack((data,test_session_gender_bow,test_session_signup_method_bow,test_session_language_bow,test_session_affiliate_channel_bow,test_session_affiliate_provider_bow,test_session_first_affiliate_tracked_bow,test_session_signup_app_bow,test_session_first_device_type_bow,test_session_first_browser_bow,test_session_unique_device_type_bow,test_session_action_tfidf,test_session_action_type_tfidf,test_session_action_detail_tfidf)).tocsr()\n",
    "    print('Vectorisation finished...')\n",
    "    print('Data ready for modelling....')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIRST FINAL :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/krutarthhd/airbnb-eda-and-xgboost\n",
    "\n",
    "def final_method(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Function takes in raw data as input.\n",
    "    and returns predictions for the input. \n",
    "    Here the input can be a single point or a set of points.\n",
    "    \n",
    "    parameters: raw data\n",
    "    \n",
    "    returns :  prediction for the data \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    country_list = []\n",
    "    start = time.process_time()\n",
    "    print('START Preprocessing....\\n')\n",
    "    preprocess_data(data)\n",
    "    print('\\n START Vectorisation...\\n')\n",
    "    data1 = vectorise_data(data)\n",
    "    \n",
    "    print('\\n START Modelling...\\n')\n",
    "    \n",
    "    rf_pred = clf_rf.predict_proba(data1)\n",
    "    \n",
    "    \n",
    "    for i in rf_pred:\n",
    "        \n",
    "        country_list.append(le.inverse_transform(np.argsort(i)[::-1][:5]).tolist())\n",
    "        \n",
    "    print('\\n PREDICTIONS....\\n')\n",
    "    print(country_list,rf_pred)\n",
    "    print('Time Taken (in secs) to return the prediction : ',time.process_time() - start)\n",
    "    \n",
    "    return country_list,rf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START Preprocessing....\n",
      "\n",
      "\"date_account_created\" field is finished processing...\n",
      "\"age\" field is finished processing...\n",
      "\"first_affiliate_tracked\" field is finished processing...\n",
      "session fields are finished processing...\n",
      "Fields already processed are dropped...\n",
      "\n",
      " START Vectorisation...\n",
      "\n",
      "Gender column vectorisation finished...\n",
      "signup_method column vectorisation finished...\n",
      "language column vectorisation finished...\n",
      "affiliate_channel column vectorisation finished...\n",
      "affiliate_provider column vectorisation finished...\n",
      "first_affiliate_tracked column vectorisation finished...\n",
      "signup_app column vectorisation finished...\n",
      "signup_app column vectorisation finished...\n",
      "first_browser column vectorisation finished...\n",
      "unique_device_type column vectorisation finished...\n",
      "unique_action column vectorisation finished...\n",
      "unique_action_type column vectorisation finished...\n",
      "unique_action_detail column vectorisation finished...\n",
      "Current Columns : \n",
      "Index(['gender', 'age', 'signup_method', 'signup_flow', 'language',\n",
      "       'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked',\n",
      "       'signup_app', 'first_device_type', 'first_browser',\n",
      "       'country_destination', 'account_created_day', 'account_created_month',\n",
      "       'account_created_year', 'Total_secs_elapsed', 'Mean_secs_elapsed',\n",
      "       'session_count', 'unique_action', 'unique_action_type',\n",
      "       'unique_action_detail', 'unique_device_type'],\n",
      "      dtype='object')\n",
      "Dropping columns that are vectorised : \n",
      "Index(['age', 'signup_flow', 'account_created_day', 'account_created_month',\n",
      "       'account_created_year', 'Total_secs_elapsed', 'Mean_secs_elapsed',\n",
      "       'session_count'],\n",
      "      dtype='object')\n",
      "(5, 8)\n",
      "Vectorisation finished...\n",
      "Data ready for modelling....\n",
      "\n",
      " START Modelling...\n",
      "\n",
      "\n",
      " PREDICTIONS....\n",
      "\n",
      "[['NDF', 'US', 'other', 'FR', 'IT'], ['NDF', 'US', 'other', 'FR', 'ES'], ['US', 'NDF', 'other', 'FR', 'GB'], ['NDF', 'US', 'other', 'FR', 'ES'], ['NDF', 'GB', 'US', 'other', 'FR']] [[3.03075093e-03 6.14493936e-03 5.81911269e-03 1.10042632e-02\n",
      "  2.72836748e-02 9.38587058e-03 1.52519248e-02 4.55363913e-01\n",
      "  1.29858500e-03 6.44631348e-04 2.90252184e-01 1.74520151e-01]\n",
      " [4.29390135e-04 9.27176021e-04 1.77615237e-03 4.71914578e-03\n",
      "  9.50782902e-03 1.73359150e-03 3.05573697e-03 8.24097828e-01\n",
      "  4.58128328e-04 6.91938715e-05 1.32661169e-01 2.05646594e-02]\n",
      " [2.00821814e-03 1.52461798e-02 4.15913418e-03 1.35162281e-02\n",
      "  5.09786144e-02 1.82416172e-02 1.72156795e-02 3.66729280e-01\n",
      "  2.70715079e-03 5.88991799e-03 4.19352207e-01 8.39557736e-02]\n",
      " [1.97871038e-03 4.10718856e-03 3.12179336e-03 1.20357950e-02\n",
      "  1.93774666e-02 6.74459131e-03 9.94104938e-03 7.59985357e-01\n",
      "  3.10165559e-03 8.69512103e-04 1.47125017e-01 3.16118638e-02]\n",
      " [2.13654064e-04 5.17779247e-04 1.46647736e-04 2.12099066e-03\n",
      "  2.96541787e-03 1.48762729e-01 1.17119892e-03 7.69050215e-01\n",
      "  3.68507684e-04 3.59597334e-05 5.79210903e-02 1.67258099e-02]]\n",
      "Time Taken (in secs) to return the prediction :  8.921875\n"
     ]
    }
   ],
   "source": [
    "country_list,rf_pred = final_method(X_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECOND FINAL :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def final_method2(raw_label,pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Function takes raw label and corresponding prediction as input and returns ndcg score.\n",
    "    \n",
    "    parameters:  ground truth , prediction\n",
    "    \n",
    "    return : ndcg score \n",
    "        \n",
    "    \"\"\"\n",
    "    print('NDCG SCORE : \\n')\n",
    "    score = ndcg_score(raw_label,pred,5)\n",
    "    print(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG SCORE : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7261859507142916"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = final_method2(Y_raw,rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have loaded the train and session raw data. Since the data is present intwo different csv files, we merged the two files together. We created a subset of this merged data and passed it as raw data into the first Final_Method.\n",
    "\n",
    "Along with this, we have loaded the vectorisers and the best model that we have saved earlier.\n",
    "\n",
    "In that function, we have performed all the steps that we did in our preproccessing of the train data. We vectorised this proccessed dataset and created the new features. Finally we have used the best model to give the predictions. \n",
    "\n",
    "We have also shown the time taken to perform all these steps and have managed to keep the time very small (8 secs).\n",
    "\n",
    "In the second Final_Method , we have passed the ground truth and the predictions and it return the NDCG Score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
